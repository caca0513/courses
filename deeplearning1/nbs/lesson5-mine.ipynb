{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "#from theano.sandbox import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import importlib\n",
    "from datetime import datetime\n",
    "\n",
    "#from utils36 import plots\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils36\n",
    "from utils36 import *\n",
    "#importlib.reload(utils36)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.data_utils import get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file data\\imdb\\models already exists.\n"
     ]
    }
   ],
   "source": [
    "#change slash in path to use in windows\n",
    "model_path = 'data\\imdb\\models'\n",
    "\n",
    "%mkdir $model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to look at the IMDB dataset, which contains movie reviews from IMDB, along with their sentiment. Keras comes with some helpers for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "idx = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the word list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'and', 'a', 'of', 'to', 'is', 'br', 'in', 'it', 'i']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_arr = sorted(idx, key=idx.get)\n",
    "idx_arr[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and this is the mapping from id to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx2word = {v: k for k, v in idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the reviews using code copied from keras.datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = get_file('imdb_full.pkl',\n",
    "                origin='https://s3.amazonaws.com/text-datasets/imdb_full.pkl',\n",
    "                md5_hash='d091312047c43cf9e4e38fef92437263')\n",
    "f = open(path, 'rb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, labels_train), (x_test, labels_test) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the 1st review. As you see, the words have been replaced by ids. The ids can be looked up in idx2word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'23022, 309, 6, 3, 1069, 209, 9, 2175, 30, 1, 169, 55, 14, 46, 82, 5869, 41, 393, 110, 138, 14, 5359, 58, 4477, 150, 8, 1, 5032, 5948, 482, 69, 5, 261, 12, 23022, 73935, 2003, 6, 73, 2436, 5, 632, 71, 6, 5359, 1, 25279, 5, 2004, 10471, 1, 5941, 1534, 34, 67, 64, 205, 140, 65, 1232, 63526, 21145, 1, 49265, 4, 1, 223, 901, 29, 3024, 69, 4, 1, 5863, 10, 694, 2, 65, 1534, 51, 10, 216, 1, 387, 8, 60, 3, 1472, 3724, 802, 5, 3521, 177, 1, 393, 10, 1238, 14030, 30, 309, 3, 353, 344, 2989, 143, 130, 5, 7804, 28, 4, 126, 5359, 1472, 2375, 5, 23022, 309, 10, 532, 12, 108, 1470, 4, 58, 556, 101, 12, 23022, 309, 6, 227, 4187, 48, 3, 2237, 12, 9, 215'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join(map(str, x_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first word of the first review is 23022. Let's see what that is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word[23022]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the whole review, mapped from ids to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my 35 years in the teaching profession lead me to believe that bromwell high's satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers' pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled at high a classic line inspector i'm here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isn't\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([idx2word[o] for o in x_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are 1 for positive, 0 for negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce vocab size by setting rare words to max index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "\n",
    "trn = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in x_train]\n",
    "test = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at distribution of lengths of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = map(len, trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([4999,  309,    6,    3, 1069,  209,    9, 2175,   30,    1,  169,   55,   14,   46,   82,\n",
       "        4999,   41,  393,  110,  138,   14, 4999,   58, 4477,  150,    8,    1, 4999, 4999,  482,\n",
       "          69,    5,  261,   12, 4999, 4999, 2003,    6,   73, 2436,    5,  632,   71,    6, 4999,\n",
       "           1, 4999,    5, 2004, 4999,    1, 4999, 1534,   34,   67,   64,  205,  140,   65, 1232,\n",
       "        4999, 4999,    1, 4999,    4,    1,  223,  901,   29, 3024,   69,    4,    1, 4999,   10,\n",
       "         694,    2,   65, 1534,   51,   10,  216,    1,  387,    8,   60,    3, 1472, 3724,  802,\n",
       "           5, 3521,  177,    1,  393,   10, 1238, 4999,   30,  309,    3,  353,  344, 2989,  143,\n",
       "         130,    5, 4999,   28,    4,  126, 4999, 1472, 2375,    5, 4999,  309,   10,  532,   12,\n",
       "         108, 1470,    4,   58,  556,  101,   12, 4999,  309,    6,  227, 4187,   48,    3, 2237,\n",
       "          12,    9,  215]),\n",
       " array([4999,   39, 4999,   14,  739, 4999, 3428,   44,   74,   32, 1831,   15,  150,   18,  112,\n",
       "           3, 1344,    5,  336,  145,   20,    1,  887,   12,   68,  277, 1189,  403,   34,  119,\n",
       "         282,   36,  167,    5,  393,  154,   39, 2299,   15,    1,  548,   88,   81,  101,    4,\n",
       "           1, 3273,   14,   40,    3,  413, 1200,  134, 4999,   41,  180,  138,   14, 3086,    1,\n",
       "         322,   20, 4930, 4999,  359,    5, 3112, 2128,    1, 4999, 4999,   39, 4999,   45, 3661,\n",
       "          27,  372,    5,  127,   53,   20,    1, 1983,    7,    7,   18,   48,   45,   22,   68,\n",
       "         345,    3, 2131,    5,  409,   20,    1, 1983,   15,    3, 3238,  206,    1, 4999,   22,\n",
       "         277,   66,   36,    3,  341,    1,  719,  729,    3, 3865, 1265,   20,    1, 1510,    3,\n",
       "        1219,    2,  282,   22,  277, 2525,    5,   64,   48,   42,   37,    5,   27, 3273,   12,\n",
       "           6, 4999, 4999, 2034,    7,    7, 3771, 3225,   34, 4186,   34,  378,   14, 4999,  296,\n",
       "           3, 1023,  129,   34,   44,  282,    8,    1,  179,  363, 4999,    5,   94,    3, 2131,\n",
       "          16,    3, 4999, 3005, 4999, 4999,    5,   64,   45,   26,   67,  409,    8,    1, 1983,\n",
       "          15, 3261,  501,  206,    1, 4999,   45, 4999, 2877,   26,   67,   78,   48,   26,  491,\n",
       "          16,    3,  702, 1184,    4,  228,   50, 4505,    1, 4999,   20,  118, 4999,    6, 1373,\n",
       "          20,    1,  887,   16,    3, 4999,   20,   24, 3964,    5, 4999,   24,  172,  844,  118,\n",
       "          26,  188, 1488,  122,    1, 4999,  237,  345,    1, 4999, 4999,   31,    3, 4999,  100,\n",
       "          42,  395,   20,   24, 4999,  118, 4999,  889,   82,  102,  584,    3,  252,   31,    1,\n",
       "         400,    4, 4787, 4999, 1962, 3861,   32, 1230, 3186,   34,  185, 4310,  156, 2325,   38,\n",
       "         341,    2,   38, 4999, 4999, 2231, 4846,    2, 4999, 4999, 2610,   34,   23,  457,  340,\n",
       "           5,    1, 1983,  504, 4355, 4999,  215,  237,   21,  340,    5, 4468, 4999, 4999,   37,\n",
       "          26,  277,  119,   51,  109, 1023,  118,   42,  545,   39, 2814,  513,   39,   27,  553,\n",
       "           7,    7,  134,    1,  116, 2022,  197, 4787,    2, 4999,  283, 1667,    5,  111,   10,\n",
       "         255,  110, 4382,    5,   27,   28,    4, 3771, 4999, 4999,  105,  118, 2597,    5,  109,\n",
       "           3,  209,    9,  284,    3, 4325,  496, 1076,    5,   24, 2761,  154,  138,   14, 4999,\n",
       "        4999,  182, 4999,   39, 4999,   15,    1,  548,    5,  120,   48,   42,   37,  257,  139,\n",
       "        4530,  156, 2325,    9,    1,  372,  248,   39,   20,    1,   82,  505,  228,    3,  376,\n",
       "        2131,   37,   29, 1023,   81,   78,   51,   33,   89,  121,   48,    5,   78,   16,   65,\n",
       "         275,  276,   33,  141,  199,    9,    5,    1, 3273,  302,    4,  769,    9,   37, 4999,\n",
       "         275,    7,    7,   39,  276,   11,   19,   77, 4999,   22,    5,  336,  406]),\n",
       " array([ 527,  117,  113,   31, 4999, 1962, 3861,  115,  902, 4999,  758,   10,   25,  123,  107,\n",
       "           2,  116,  136,    8, 1646, 4999,   23,  330,    5,  597,    1, 4999,   20,  390,    6,\n",
       "           3,  353,   14,   49,   14,  230,    8, 4999, 4999,    1,  190,   20, 4999,    6,   79,\n",
       "         894,  100,  109, 3609,    4,  109,    3, 4999, 3485,   43,   24, 1407,    2,  109, 4999,\n",
       "           1, 2405,    4, 4999, 4999, 4999, 4999,  143,    3, 2405,   26,  557,  286,  160,  712,\n",
       "        4122, 4999,    3,  511,   36,    1,  300, 2793, 4999,  120,    6,  774,  130,   96,   14,\n",
       "           3, 1165, 4999,   34,  491,    5, 4263,    1, 4999,   24,  106,    6,   50, 4999,   71,\n",
       "         641,    1, 1547,  133,    2,    1,  133,  118,    1, 3273, 4999,    3, 4999, 2135,   23,\n",
       "          29,   55, 2236,  165,   15,    1, 2974,  133,    2,    1,  104,  191, 4999,  994,   28,\n",
       "        4999,   11,   17,  211,  125,  254,   55,   10,   64,    9,   60,    6,  176,  397]),\n",
       " array([  11,    6,  711,    1,   88, 2181,   19, 4999,    1, 3225, 4999,  249,   91, 3045,    9,\n",
       "         124,   21,  199,    3,  818,  647,    4, 4999, 1022,  132,   86, 3842, 3558,  517,    3,\n",
       "         818,  647,    4, 4999, 4999,   39, 2743,  517,    3,  818,  647,    4, 4999,   22, 3752,\n",
       "         108,    4,    1,  637,  806, 1032,   18,  128,   11,   19,    6,   52, 3201,    8,    3,\n",
       "          93,  108, 1287,   23,   21,    2,    5, 1595,   12,  122,    8,    3,   62,   41,   46,\n",
       "           4,    1,   88, 4999, 4999, 1063,    4,  923,    6,  368, 1156,   91,   21,    1, 3870,\n",
       "         708,   18,   91,   21,  592,  342,   58,   61, 3303,    6,   12, 3225,  141,   25,  174,\n",
       "         291,  331,    8,    1,  482,   10,  116, 3771,   14,    3,  164,    2,  561,   21,   35,\n",
       "          73,   14,    3,  482]),\n",
       " array([  11,    6,   21,    1,  798, 3771, 3225,   19,    9,   13,   73,  326, 2761,   71,   88,\n",
       "           4,   24,   99,    2,  162,   66,    3,  111,   12,   13, 4999, 2811, 1962, 3861,   90,\n",
       "           1,   17,   56,    6,  138,    3,  774,  464, 1147,  521,   47,   68,   46,  385,   12,\n",
       "          97,   25,   74, 4999,   43,    3,  224,   50,    2,   46,  136,   12,   97,  239,   25,\n",
       "          74,  602,    5,   94,    1,  670,    5,   78,   35,   18,   29,    8,   29,   11,    6,\n",
       "         287,    1, 1863,    5,  848,    2,   64,    9,    1,  113,   13,   49,  441, 3225,  306,\n",
       "         119,    3,   49,  289,  206,   24, 4999, 1383,    5, 2552,    5,    1,  308,  171, 3861,\n",
       "          13,    1,  115,  281,    8,    1,   17,   18, 4999,    2, 4999,  196,  253,   65,  528,\n",
       "          70]),\n",
       " array([  11,  215,    1, 1714, 2160, 1698,  882,    6,    9,    1, 2809, 2137, 2160, 1698,    4,\n",
       "        1133,  705, 2243,   11,    6,    3, 4999,    4,    1,  353,  450,  206,  117, 4999, 1846,\n",
       "          16, 4999,  159,  116,    4,    1,  705,   18,   11,  215,    3,  705, 3110, 4999,   11,\n",
       "           6,   50,    3,  733,  833, 2193,  140,   60, 1698, 1024,    5, 4999,    3, 1192,  427,\n",
       "           2,   24, 4999,    7,    7,   79, 1181, 4602,  446,    2, 4999, 4999,   11,  833,  450,\n",
       "         296,  181,   73,   37,    3, 1633, 4433,  363, 4999,  106,  211,  488,    5, 4999,   24,\n",
       "        3356,    7,    7,   10,  212,  132,   12,   10,   13,  542, 2173,  148,   11,   17,  993,\n",
       "           5, 3333, 3616, 4999,   39, 4999,    9,  418,   50,   37,   10,   13,  146,    3,  229,\n",
       "        1698,   14,   26,   13,  162, 3459,    1, 1726,   36,    3,  837,  412, 1968,    8,   82,\n",
       "         712,    9,  418,  144,    2,   10,   13,  499,    5, 4999,    5,    1,  860,    4,    1,\n",
       "          62,    7,    7,   29,    8,   29,   42,  287,    3,  103,  148,   42,  404,   21, 2550,\n",
       "        2321,  311, 2393,    7,    7,    9, 4999,    3,  690,  690,  155,   36,    7,    7,    1,\n",
       "        4999]),\n",
       " array([ 419,   91,   32,  495,    5, 2973,   94,    3,  547, 1782,  705,    7,    7,    1,   62,\n",
       "        4183,    8,  324, 4999,  134,   22,   89,   57, 1492,    9, 1445,    7,    7,  475,  236,\n",
       "          31, 2160, 1698,    1, 3121, 2442,    8,    1,   19,   67,  303, 1741,    2,   67,  239,\n",
       "        4522,   86,   73,   22,  355,    1,   19,  187,    1, 2023,  111,    6,   52, 1725,    1,\n",
       "          17,  149, 3406, 1643,   22,    2,  128, 4999,   22,  192,    5,  398,   22, 1532,    1,\n",
       "         455,    6,   49,  358,    4, 2687,    5, 2712, 4627, 4999,    4,  833,    2, 4999,    6,\n",
       "          49,    7,    7,   52,  324,  297,   55,  103,   45,   22,   23,  264,    5, 4582,  142,\n",
       "           2,  839,    3, 3014,  343,   62]),\n",
       " array([   8,   11, 4999, 4999, 1984,  705,  445,   20,  280,  684, 4868, 2160, 1698,    3, 4999,\n",
       "         561,    2,  519,  311,  737,  120, 3229,  458, 4999,   31,    1, 4999,   62,    4,    3,\n",
       "         182, 4999,    2,   24, 4999,  449, 4999, 4999,   51, 4999, 1201, 4999,   41,   11, 4999,\n",
       "          62,  187, 4868,  656,  306, 1306,   80,    3, 4999,  733,   12, 4999,    3, 2492, 4999,\n",
       "        1790,    5,  595, 4116, 3932,    7,    7,   22,   63,  141,  567,  883,  131,  792,    2,\n",
       "         103,    1,   19,  147,    7,    7,    1,   86,  119,   26, 1582,   24, 3964,  274,   16,\n",
       "        1560, 4999, 3598,   38,  159,  110,  141,   27, 4999,  122,    2, 1409,    5, 4999,  136,\n",
       "        1538,   42, 4999,    1,  280,  873,    4,   38, 1745,    2, 1749, 4999,  141,   27,  575,\n",
       "          31,    1,   55,  440, 1698, 1744,    5,  159,  779,  866,   38, 4999,   97,   27,    8,\n",
       "         885,   18,    3, 3408,   97,   25,   27,   90,  810,    8,  342,    1, 4999,   39,  371,\n",
       "        2211,  136,    1,   19,   59, 4272,   36,    3,  793,  799,   86,   41,    3, 2027,  602,\n",
       "           7,    7, 1698,    2, 3468, 4999,   14, 4999,   89,  303, 2718,  864,   14,    3,  375,\n",
       "           3,  133,   39,  104, 4999,   65,  646,  235,   25, 1675,  267,    1,  865,  897,    1,\n",
       "         174,    6, 4999, 1698, 1577,   32, 4840,  562, 3585,    2,   21,    3,  989, 4999, 4602,\n",
       "         446,   14, 2244,  911, 4999,   14, 4999,    2, 4999, 4999, 4999, 4999,   23,   29,  401,\n",
       "           7,    7,  115,    4,   29, 4999, 4204, 3243,    8,    1,  945, 2367,    4, 2243, 1560,\n",
       "         446,    6, 2293,    8,  657, 4999,  235,   27,   22,  121,   37,   12,  229,   36, 4999,\n",
       "          47,   25,   74,  447,  150,   51, 4999,  740,  113, 2125,  465,    5, 2098,   15,  369,\n",
       "         685,    5,    3, 4999, 4999,    4,  552,  431,   33,   97,   25, 1922, 4999,   16,   46,\n",
       "        1341, 4999,   56,    6,   12,   49,    2,  164, 2326, 4999,  404, 4999, 2909,   26,   57,\n",
       "         163,  394,    3, 4999,   36,    3, 4999, 1689, 2567,    7,    7,  414,  924, 4999, 4999,\n",
       "        4999,    2, 3970, 2545, 1830, 4999,   36, 2814, 4999, 2584,    7,    7,    1,  311, 4999,\n",
       "         297, 4999, 4999, 2326, 4999, 2160, 1698, 4999, 4999, 4602,  446, 4999, 4999]),\n",
       " array([   1,  311, 4999, 2942,  297,  238, 2160, 1698, 4999, 4999, 3468, 4999, 4999, 4999,  911,\n",
       "        4999, 4602,  446,  305, 4999, 2748, 4999, 4999, 1962, 3368, 4999, 2326, 4999,    7,    7,\n",
       "        4999, 4999,  405, 1698,    3,  756,   43,  361, 1314,  236,    7,    7,   48,    6,    9,\n",
       "          41, 4999,    2,  448,   48,    6,    1,  748, 4578,   28, 4999,   16,    1,   82,    2,\n",
       "         135,    6,    9,  217,    1, 4999,    7,    7,    8,    1, 2481, 4999,  334, 2675,  445,\n",
       "          20,  280,  684,   54,  326, 1698,  378,   14,    3,  737, 1875, 1610,  770, 4868,   54,\n",
       "          28,   34, 4532,  534,  237, 4999,  117,    1, 4999,    2,   44, 4999,   32,  218,  334,\n",
       "           8,    1,  809,    4,    3,  182,  427,  770, 4999, 4999, 4999,   34,   44, 4999,    3,\n",
       "        4999,   41,    1, 4999,    4,   24, 3203, 1937,    5,   54, 2063, 3791, 4999, 4999,   34,\n",
       "         405,    9,    5,   54,   28,    5,  329,   15,  306,    7,    7,   54,   28,    6, 1955,\n",
       "        4011,   18, 1113, 3701,   41,    1, 4999, 2008,    4, 4999,  109, 4999,    2, 3372, 4999,\n",
       "          15,  150,  363,   26,   13,  414, 4999,   31,    3, 3730,  770, 4204, 4999,  740,   32,\n",
       "         318,  236,   34,   44, 4999,    1,  427,   18,   38, 4999,   16,   54,   28, 2669,   12,\n",
       "        4999,    6, 1718,   36, 4419, 1955,   54,   28,  491,    5,  906,    1,  448,   18,    6,\n",
       "        1084,    8,  821,    5,   65,  866, 4999, 4999, 4201,   51,    1, 4446,    6, 4999,   31,\n",
       "          24, 4999, 1458, 4999, 4999,  622, 2114, 4999,   36,   65,  159,  779,  540, 1600,   44,\n",
       "          54,   28,    8,   32,  918, 4999,   12,   44,   61,  147, 2068,   80,    3, 4999,    8,\n",
       "           3, 4999,   51,   26, 1065,    5,   78,   46, 4999,   80, 4204,    2, 4999, 4257, 4999,\n",
       "          46, 4999,   12,   26,  158, 4999,    7,    7,  395,   31, 4999, 4999,   34,  998, 1037,\n",
       "           1,  878,   16,   24, 1135, 1458, 3970, 2545,    2,    1,  595, 4999,  164, 4999,    2,\n",
       "         445,   20,    3,  280,   62,   41,    3, 4999, 4999,  255,   43,   44,   46, 4999,  385,\n",
       "          12,  518,   20,  365, 4999,   37,   98,   49,  151, 3354, 4357, 4999,  124,    9, 1522,\n",
       "          12, 1698,  405,    3,  756,   43,  361, 1314,  236,   14,    1, 4999,   49, 2284, 1610,\n",
       "          34, 2067,  491,    5,  261,   12,   24,  609,   28,  334,    6,    8,  189,  144,    2,\n",
       "         124,  116,   87,    1,   28,  152,   12,   44, 3951,   24,  202,  632,    2,   44,   46,\n",
       "        4330, 2147,  385,   16,    1,  945, 4999,  622,   28, 1745, 4999,   10,   77,  560, 4999,\n",
       "          18, 4999,    1, 4179,    4,   38,  106,   12,   67, 4999,   22,    5,    1, 2023,    7,\n",
       "           7,  187,    1,   19, 1126,   43,    4, 2548,    2,  850,  458,    3,  224, 3608,    2,\n",
       "         724,  463,    3, 4999,  523,  415,    4, 4999,    2,  733,   31, 4999,    9, 4101,    5,\n",
       "        1629,    5,  126,  202, 2416,  541,   27, 4687,    4,   48,   22,  437,   15]),\n",
       " array([  22,  121, 2160, 1698,  555, 4999,   87,    6, 1340, 1202,  306,    8,    1, 2017, 4438,\n",
       "          16,   29,  131,  992, 1287,   26,   44,  221,   11, 2065,   16,  379,    1, 1398,    4,\n",
       "         338,    5, 4999,   60, 4999,   51,    9,  382,   43,   18,    6,  147,    3, 1212,  353,\n",
       "           1, 3284,   26,   44,   90, 4438,   25,   74,  774,  259, 4999,    2,   28,  531, 4880,\n",
       "           1,  311, 4999,  463, 1498,  854,    2,    3, 1602,  285,  763,    6,  790,   24,  115,\n",
       "         154,  807,    7,    7,   11,    6,    3,   52, 2857,   62,   57,  148,    9,  149, 1467,\n",
       "           3, 1432,  452,   39,  256,   12, 3102, 1821,   15,   12,  548,    1, 1117,    4,    1,\n",
       "          19,    6,  445,   20,   32,  776,  417,    4, 4999,   12,  128,   44,  243,    5,   27,\n",
       "        4999, 4999,    8,  309,  393,   10,  329,   32, 4999,   31,    3,  503,  770, 2044, 4999,\n",
       "        2817,   34, 3114, 2990, 2566,    2,  850, 4999, 4419,   14,    3,  956,   10,   13, 1678,\n",
       "          31,    1,   62,  363,   10,  329, 4999, 4689,   12, 2817,  200,   21,  162, 1775,   51,\n",
       "          10,  216,   11,   17,    1, 1468, 1414,   12, 2160, 1698,   35, 2102,  997, 4999,    8,\n",
       "          58,  327,    7,    7, 4999, 4999,  239,  405,   38,  115,  902,  236,   96,   14,    1,\n",
       "        1113, 4999, 4999,   38,  214,   13,    3,  227, 1412,   36,  145,   56,   66,    8,   99,\n",
       "          37,  114,  714, 3903,   47,   68,   57,  208,   56,  607,   80,    1,  367,  118,   10,\n",
       "         194,   56,   13, 4485,  205,   30,   69,    9,  301,    3,   49,  521,    5,  294,   12,\n",
       "         429,    4,  214,    2,   42,   11, 4635,  243,   70, 4999,  214,   12,  163, 4999, 4999,\n",
       "         239,   28,    4,    1,  115, 1504,    4,   11, 2245,   21,    5,   25,   57,   74, 2302,\n",
       "          15,   32, 1806, 1341,   14,    4, 4999,   42, 1045,   12,   47,    6,   30,  219,   28,\n",
       "         252,    8,   11,  179,   34,    6,   37,   11,    2,   42,  626,   96,    7,    7,   11,\n",
       "           6,    3,   49,  462,   19,   12,   10,  542,  383,   27, 2845,    5,   27, 4999,  148,\n",
       "          85,   11,   17,  886,   22,   16,    3,  677,  544,   30,    1,  127])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'map' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-f71b15918c97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtemp1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'map' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "temp1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2493, 10, 237.71364)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array(list(map(len, trn)))\n",
    "(lens.max(), lens.min(), lens.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad (with zero) or truncate each sentence to make consistent length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 500\n",
    "\n",
    "trn = sequence.pad_sequences(trn, maxlen=seq_len, value=0)\n",
    "test = sequence.pad_sequences(test, maxlen=seq_len, value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in nice rectangular matrices that can be passed to ML algorithms. Reviews shorter than 500 words are pre-padded with zeros, those greater are truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 500)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Single hidden layer NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The simplest model that tends to give reasonable results is a single hidden layer net. So let's try that. Note that we can't expect to get any useful results by feeding word ids directly into a neural net - so instead we use an embedding to replace them with a vector of 32 (initially random) floats for each word in the vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, 32, input_length=seq_len),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               1600100   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,760,201\n",
      "Trainable params: 1,760,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\ML\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 6s 231us/step - loss: 0.4798 - acc: 0.7399 - val_loss: 0.2868 - val_acc: 0.8794\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 4s 152us/step - loss: 0.2047 - acc: 0.9230 - val_loss: 0.2977 - val_acc: 0.8763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2147d0fa198>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The [stanford paper](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf) that this dataset is from cites a state of the art accuracy (without unlabelled data) of 0.883. So we're short of that, but on the right track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Single conv layer with max pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A CNN is likely to work better, since it's designed to take advantage of ordered data. We'll need to use a 1D CNN, since a sequence of words is 1D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv1 = Sequential([\n",
    "    Embedding(vocab_size, 32, input_length=seq_len),\n",
    "    Dropout(0.2),\n",
    "    Conv1D(64, 5, padding='same', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv1.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\ML\\Anaconda3\\envs\\dlwin36\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 9s 359us/step - loss: 0.4980 - acc: 0.7150 - val_loss: 0.2791 - val_acc: 0.8860\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 8s 313us/step - loss: 0.2490 - acc: 0.9058 - val_loss: 0.2566 - val_acc: 0.8938\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 8s 314us/step - loss: 0.1994 - acc: 0.9269 - val_loss: 0.2693 - val_acc: 0.8893\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 8s 311us/step - loss: 0.1665 - acc: 0.9408 - val_loss: 0.2711 - val_acc: 0.8882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21406656a90>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.fit(trn, labels_train, validation_data=(test, labels_test), epochs=4, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 8s 316us/step - loss: 0.1403 - acc: 0.9514 - val_loss: 0.3104 - val_acc: 0.8852\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 8s 313us/step - loss: 0.1171 - acc: 0.9594 - val_loss: 0.3191 - val_acc: 0.8854\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 8s 314us/step - loss: 0.0949 - acc: 0.9665 - val_loss: 0.3732 - val_acc: 0.8834\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 8s 313us/step - loss: 0.0781 - acc: 0.9717 - val_loss: 0.3917 - val_acc: 0.8818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2147d0fa1d0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.fit(trn, labels_train, validation_data=(test, labels_test), epochs=4, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "That's well past the Stanford paper's accuracy - another win for CNNs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv1.save_weights(model_path + 'conv1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv1.load_weights(model_path + 'conv1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Pre-trained vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You may want to look at wordvectors.ipynb before moving on.\n",
    "\n",
    "In this section, we replicate the previous CNN, but using pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I:\\\\ML\\\\courses\\\\deeplearning1\\\\nbs\\\\data\\\\glove\\\\results'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_path = os.path.abspath('data\\glove\\\\results')\n",
    "glove_path\n",
    "#%mkdir $glove_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_dataset(dataset):\n",
    "    \"\"\"Download the requested glove dataset from files.fast.ai\n",
    "    and return a location that can be passed to load_vectors.\n",
    "    \"\"\"\n",
    "    # see wordvectors.ipynb for info on how these files were\n",
    "    # generated from the original glove data.\n",
    "    md5sums = {'6B.50d': '8e1557d1228decbda7db6dfd81cd9909',\n",
    "               '6B.100d': 'c92dbbeacde2b0384a43014885a60b2c',\n",
    "               '6B.200d': 'af271b46c04b0b2e41a84d8cd806178d',\n",
    "               '6B.300d': '30290210376887dcc6d0a5a6374d8255'}\n",
    "    glove_path = os.path.abspath('data\\glove\\\\results')\n",
    "    %mkdir $glove_path\n",
    "    return get_file(dataset,\n",
    "                    'http://files.fast.ai/models/glove/' + dataset + '.tgz',\n",
    "                    cache_subdir=glove_path,\n",
    "                    md5_hash=md5sums.get(dataset, None),\n",
    "                    untar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_vectors(loc):\n",
    "    return (load_array(loc+'.dat'),\n",
    "        pickle.load(open(loc+'_words.pkl','rb'), encoding='latin1'),\n",
    "        pickle.load(open(loc+'_idx.pkl','rb'), encoding='latin1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vecs, words, wordidx = load_vectors(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The glove word ids and imdb word ids use different indexes. So we create a simple function that creates an embedding matrix using the indexes from imdb, and the embeddings from glove (where they exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_emb():\n",
    "    n_fact = vecs.shape[1]\n",
    "    emb = np.zeros((vocab_size, n_fact))\n",
    "\n",
    "    for i in range(1,len(emb)):\n",
    "        word = idx2word[i]\n",
    "        if word and re.match(r\"^[a-zA-Z0-9\\-]*$\", word):\n",
    "            src_idx = wordidx[word]\n",
    "            emb[i] = vecs[src_idx]\n",
    "        else:\n",
    "            # If we can't find the word in glove, randomly initialize\n",
    "            emb[i] = normal(scale=0.6, size=(n_fact,))\n",
    "\n",
    "    # This is our \"rare word\" id - we want to randomly initialize\n",
    "    emb[-1] = normal(scale=0.6, size=(n_fact,))\n",
    "    emb/=3\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "emb = create_emb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We pass our embedding matrix to the Embedding constructor, and set it to non-trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, 50, input_length=seq_len, \n",
    "              weights=[emb], trainable=False),\n",
    "    Dropout(0.25),\n",
    "    Conv1D(64, 5, padding='same', activation='relu'),\n",
    "    Dropout(0.25),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 8s 317us/step - loss: 0.6369 - acc: 0.6275 - val_loss: 0.5410 - val_acc: 0.7385\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 7s 298us/step - loss: 0.5062 - acc: 0.7624 - val_loss: 0.4579 - val_acc: 0.8030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x215451c39e8>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 7s 297us/step - loss: 0.4574 - acc: 0.7936 - val_loss: 0.4302 - val_acc: 0.7991\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 7s 296us/step - loss: 0.4272 - acc: 0.8058 - val_loss: 0.4227 - val_acc: 0.8056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2151d1d0160>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 7s 299us/step - loss: 0.4065 - acc: 0.8187 - val_loss: 0.4044 - val_acc: 0.8196\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 7s 297us/step - loss: 0.3841 - acc: 0.8311 - val_loss: 0.3920 - val_acc: 0.8231\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 7s 298us/step - loss: 0.3688 - acc: 0.8360 - val_loss: 0.3818 - val_acc: 0.8305\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 7s 296us/step - loss: 0.3443 - acc: 0.8487 - val_loss: 0.3911 - val_acc: 0.8257\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 7s 296us/step - loss: 0.3381 - acc: 0.8531 - val_loss: 0.3774 - val_acc: 0.8344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x215458dcf60>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 7s 298us/step - loss: 0.3259 - acc: 0.8586 - val_loss: 0.3866 - val_acc: 0.8295\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 7s 297us/step - loss: 0.3117 - acc: 0.8657 - val_loss: 0.3768 - val_acc: 0.8351\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 7s 296us/step - loss: 0.3019 - acc: 0.8699 - val_loss: 0.3789 - val_acc: 0.8331\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 7s 298us/step - loss: 0.2973 - acc: 0.8742 - val_loss: 0.4292 - val_acc: 0.8126\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 7s 296us/step - loss: 0.2828 - acc: 0.8797 - val_loss: 0.4004 - val_acc: 0.8313\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x215458dcd30>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We already have beaten our previous model! But let's fine-tune the embedding weights - especially since the words we couldn't find in glove just have random embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 9s 352us/step - loss: 0.1894 - acc: 0.9246 - val_loss: 0.3087 - val_acc: 0.8799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x215458dceb8>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As expected, that's given us a nice little boost. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data\\\\imdb\\\\models'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+'glove50.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Multi-size CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is an implementation of a multi-size CNN as shown in Ben Bowles' [excellent blog post](https://quid.com/feed/how-quid-uses-deep-learning-with-small-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "??Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Merge'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-faf69db45dbb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMerge\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Merge'"
     ]
    }
   ],
   "source": [
    "#from keras.layers import Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We use the functional API to create multiple conv layers of different sizes, and then concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "graph_in = Input ((vocab_size, 50))\n",
    "convs = [ ] \n",
    "for fsz in range (3, 6): \n",
    "    x = Conv1D(64, fsz, padding='same', activation=\"relu\")(graph_in)\n",
    "    x = MaxPooling1D()(x) \n",
    "    x = Flatten()(x) \n",
    "    convs.append(x)\n",
    "out = keras.layers.concatenate(convs) \n",
    "graph = Model(graph_in, out) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "emb = create_emb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We then replace the conv/max-pool layer in our original CNN with the concatenated conv layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Sequential ([\n",
    "    Embedding(vocab_size, 50, input_length=seq_len, weights=[emb]),\n",
    "    Dropout (0.2),\n",
    "    graph,\n",
    "    Dropout (0.5),\n",
    "    Dense (100, activation=\"relu\"),\n",
    "    Dropout (0.7),\n",
    "    Dense (1, activation='sigmoid')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 500, 50)           250000    \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 500, 50)           0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              multiple                  38592     \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 48000)             0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               4800100   \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 5,088,793\n",
      "Trainable params: 5,088,793\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 19s 758us/step - loss: 0.6298 - acc: 0.6200 - val_loss: 0.3489 - val_acc: 0.8598\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 18s 727us/step - loss: 0.3307 - acc: 0.8651 - val_loss: 0.2682 - val_acc: 0.8888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21549ac6cf8>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Interestingly, I found that in this case I got best results when I started the embedding layer as being trainable, and then set it to non-trainable after a couple of epochs. I have no idea why!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 17s 676us/step - loss: 0.1836 - acc: 0.9322 - val_loss: 0.2769 - val_acc: 0.8970\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 16s 642us/step - loss: 0.1762 - acc: 0.9364 - val_loss: 0.2714 - val_acc: 0.8954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2154b005668>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This more complex architecture has given us another boost in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We haven't covered this bit yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, 32, input_length=seq_len, mask_zero=True, embeddings_regularizer=l2(1e-6)),\n",
    "    Dropout(0.2),\n",
    "    LSTM(100, implementation=2),\n",
    "    Dense(1, activation='sigmoid')])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 371s 15ms/step - loss: 0.4609 - acc: 0.7737 - val_loss: 0.3102 - val_acc: 0.8702\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 370s 15ms/step - loss: 0.3206 - acc: 0.8688 - val_loss: 0.3343 - val_acc: 0.8621\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 370s 15ms/step - loss: 0.2557 - acc: 0.9009 - val_loss: 0.3090 - val_acc: 0.8698\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 370s 15ms/step - loss: 0.2281 - acc: 0.9134 - val_loss: 0.3411 - val_acc: 0.8698\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 370s 15ms/step - loss: 0.1944 - acc: 0.9279 - val_loss: 0.3176 - val_acc: 0.8680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21550eadcc0>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
